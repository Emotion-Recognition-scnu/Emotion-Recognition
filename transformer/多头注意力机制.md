# 多头注意力机制

[注意力机制是一种模仿人类视觉和认知系统的方法，它允许神经网络在处理输入数据时集中注意力于相关的部分。通过引入注意力机制，神经网络能够自动地学习并选择性地关注输入中的重要信息，提高模型的性能和泛化能力](https://blog.csdn.net/abcdefg90876/article/details/104164551)[1](https://blog.csdn.net/abcdefg90876/article/details/104164551)。

[Transformer模型是一种采用自注意力机制的深度学习模型，主要用于自然语言处理和计算机视觉领域。Transformer模型由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。Transformer模型的核心是自注意力机制，它可以按输入数据各部分重要性的不同而分配不同的权重。Transformer模型的整体结构由Encoder和Decoder两个部分组成，每个部分包含6个block，其中包括多个Self-Attention，是由多个Self-Attention组成的](https://blog.csdn.net/abcdefg90876/article/details/104164551)[2](https://blog.csdn.net/qq_43703185/article/details/120304556)。

## 自注意力机制

### 自注意力机制的基本思想

在处理序列数据时，每个元素都可以与序列中的其他元素建立关联，而不仅仅是依赖于相邻位置的元素。这样可以有效地捕捉序列中的长距离依赖关系，提高模型的表达能力。

### 自注意力机制的计算过程如下：

- 首先，将输入序列的每个元素（如单词）的输入映射为一个固定长度的向量，称为**嵌入向量**（Embedding Vector）。
- 然后，对每个嵌入向量分别乘以三个不同的权重矩阵，得到三个新的向量，分别称为**查询向量**（Query Vector）、**键向量**（Key Vector）和**值向量**（Value Vector）。
- ![img](https://pic3.zhimg.com/80/v2-4f4958704952dcf2c4b652a1cd38f32e_720w.webp)
- 接着，对每个查询向量，计算它与所有键向量的点积（Dot Product），得到一个得分向量（Score Vector）。这个得分向量反映了查询向量与各个键向量的相似度，也就是输入序列中各个元素对当前元素的重要性。
- 然后，对每个得分向量进行缩放（Scale），即除以一个常数，通常是嵌入向量的维度的平方根。这样做是为了避免得分向量的值过大，导致梯度爆炸或消失。
- 接着，对每个缩放后的得分向量进行Softmax操作，得到一个注意力权重向量（Attention Weight Vector）。这个向量的每个元素都是一个介于0和1之间的数，表示输入序列中各个元素对当前元素的注意力权重，所有元素的和为1。
- ![img](https://pic2.zhimg.com/80/v2-9699a37b96c2b62d22b312b5e1863acd_720w.webp)
- ![image-20240119142638467](C:\Users\苏俊\AppData\Roaming\Typora\typora-user-images\image-20240119142638467.png)
- 最后，对每个注意力权重向量，计算它与所有值向量的加权和（Weighted Sum），得到一个输出向量（Output Vector）。这个向量是输入序列中各个元素的加权组合，反映了当前元素的上下文信息。
- ![image-20240119143419325](C:\Users\苏俊\AppData\Roaming\Typora\typora-user-images\image-20240119143419325.png)

------------

-------



## **自注意力机制中的Softmax操作和加权和操作的详细解释**

Softmax操作是一种将任意实数向量转换为概率分布向量的函数，它的作用是将向量中的每个元素压缩到0和1之间，并且保证所有元素的和为1。Softmax操作的公式如下：
$$
\text{Softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)}
$$
其中，x_i表示向量x中的第i个元素，n表示向量的长度，
$$
\exp
$$
表示指数函数，即e的幂。Softmax操作的意义是将向量中的每个元素看作一个事件的发生概率，使得所有事件的概率之和为1。Softmax操作的一个特点是，它会放大向量中较大的元素，缩小向量中较小的元素，从而突出向量中的主要特征。

例如，假设我们有一个向量x = [1, 2, 3, 4]，我们对它进行Softmax操作，得到：
$$
\text{Softmax}(x) = \left[\frac{\exp(1)}{\exp(1)+\exp(2)+\exp(3)+\exp(4)}, \frac{\exp(2)}{\exp(1)+\exp(2)+\exp(3)+\exp(4)}, \frac{\exp(3)}{\exp(1)+\exp(2)+\exp(3)+\exp(4)}, \frac{\exp(4)}{\exp(1)+\exp(2)+\exp(3)+\exp(4)}\right]
$$
计算后，得到：
$$
\text{Softmax}(x) \approx [0.032, 0.087, 0.237, 0.644]
$$
可以看到，向量x中的最大元素4被转换为了最大的概率0.644，而向量x中的最小元素1被转换为了最小的概率0.032，同时所有元素的和为1。

在自注意力机制中，我们对每个缩放后的得分向量进行Softmax操作，得到一个注意力权重向量，这个向量的每个元素都是一个介于0和1之间的数，表示输入序列中各个元素对当前元素的注意力权重，所有元素的和为1。这样，我们就可以根据输入序列中各个元素的重要性，给它们分配不同的权重，从而实现注意力机制的目的。

加权和操作是一种将两个向量相乘并求和的函数，它的作用是根据一个向量中的权重，对另一个向量中的元素进行加权组合。加权和操作的公式如下：
$$
\text{WeightedSum}(w, v) = \sum_{i=1}^n w_i v_i
$$
其中，w表示权重向量，v表示元素向量，w_i表示权重向量中的第i个元素，v_i表示元素向量中的第i个元素，n表示向量的长度。加权和操作的意义是根据权重向量中的概率分布，对元素向量中的元素进行加权平均，得到一个综合的结果。加权和操作的一个特点是，它会强调权重向量中较大的元素对应的元素向量中的元素，忽略权重向量中较小的元素对应的元素向量中的元素，从而突出元素向量中的主要特征。

例如，假设我们有一个权重向量w = [0.1, 0.2, 0.3, 0.4]，和一个元素向量v = [1, 2, 3, 4]，我们对它们进行加权和操作，得到：
$$
\text{WeightedSum}(w, v) = 0.1 \times 1 + 0.2 \times 2 + 0.3 \times 3 + 0.4 \times 4
$$
计算后，得到：
$$
\text{WeightedSum}(w, v) = 3
$$
可以看到，权重向量w中的最大元素0.4对应的元素向量v中的元素4对加权和的结果贡献最大，而权重向量w中的最小元素0.1对应的元素向量v中的元素1对加权和的结果贡献最小，同时加权和的结果是一个标量。

在自注意力机制中，我们对每个注意力权重向量，计算它与所有值向量的加权和，得到一个输出向量。这个向量是输入序列中各个元素的加权组合，反映了当前元素的上下文信息。这样，我们就可以根据注意力权重向量中的概率分布，对值向量中的元素进行加权平均，得到一个综合的输出向量，从而实现自注意力机制的目的。





-----------

---------

## 多头注意力机制

多头自注意力机制是在自注意力机制的基础上发展起来的，是自注意力机制的变体，旨在增强模型的表达能力和泛化能力。

### 多头自注意力机制的计算过程如下：

- 首先，将输入序列的每个元素映射为一个嵌入向量，与自注意力机制相同。

- 然后，对每个嵌入向量分别乘以H组不同的权重矩阵，得到H组查询向量、键向量和值向量，其中H是一个超参数，表示多头的个数。这样做是为了让模型可以从不同的角度和子空间去关注输入序列中的信息，增加模型的多样性和灵活性。

- 接着，对每组查询向量、键向量和值向量，分别进行自注意力机制的计算，得到H个输出向量，与自注意力机制相同。

- 然后，将H个输出向量拼接（Concatenate）成一个长向量，再乘以一个权重矩阵，得到一个新的输出向量。这样做是为了将不同头的输出向量融合起来，得到一个综合的输出向量。

- 最后，对新的输出向量进行层归一化（Layer Normalization）和残差连接（Residual Connection），得到最终的输出向量。这样做是为了稳定模型的训练，防止梯度消失或爆炸，同时保留输入向量的信息。

- ----

  

  您想了解的是多头自注意力机制中的拼接操作和层归一化与残差连接操作的详细介绍，我将尽量用简单的语言和例子来说明。

  拼接操作是一种将多个向量连接成一个长向量的函数，它的作用是将不同的信息组合起来，得到一个更丰富的表示。拼接操作的公式如下：
$$
  \text{Concatenate}(v_1, v_2, ..., v_H) = [v_1; v_2; ...; v_H]
$$
  其中，v_1, v_2, ..., v_H表示H个向量，[;]表示向量的连接符号，H表示多头的个数。拼接操作的意义是将H个向量中的每个元素按顺序排列在一起，形成一个新的向量，这个向量的长度是H个向量的长度之和。拼接操作的一个特点是，它会保留H个向量中的所有信息，不会丢失任何信息。

  例如，假设我们有两个向量v_1 = [1, 2, 3]和v_2 = [4, 5, 6]，我们对它们进行拼接操作，得到：
$$
  \text{Concatenate}(v_1, v_2) = [1, 2, 3, 4, 5, 6]
$$
  可以看到，拼接操作将两个向量中的所有元素按顺序连接起来，得到一个长度为6的向量，这个向量包含了两个向量中的所有信息。

  在多头自注意力机制中，我们将H个输出向量拼接成一个长向量，再乘以一个权重矩阵，得到一个新的输出向量。这样做是为了将不同头的输出向量融合起来，得到一个综合的输出向量。这个输出向量的长度与输入向量的长度相同，但是包含了不同头的输出向量中的信息，从而增强了模型的表达能力。

  层归一化是一种对向量进行规范化的函数，它的作用是将向量中的每个元素减去均值，再除以标准差，得到一个均值为0，方差为1的向量。层归一化的公式如下：
$$
  \text{LayerNorm}(x_i) = \frac{x_i - \mu}{\sigma}
$$
  其中，
$$
  x_i
$$
  表示向量x中的第i个元素，
$$
  \mu
$$
  表示向量x的均值，
$$
  \sigma
$$
  表示向量x的标准差。层归一化的意义是将向量中的每个元素转换为一个标准化的值，使得向量的分布更加均匀，更容易被神经网络处理。层归一化的一个特点是，它是对每个向量单独进行的，而不是对整个批次（Batch）的数据进行的，这样可以避免批次大小的影响，提高模型的稳定性。

  例如，假设我们有一个向量x = [1, 2, 3, 4]，我们对它进行层归一化，得到：
$$
  \text{LayerNorm}(x) = \left[\frac{1 - 2.5}{1.29}, \frac{2 - 2.5}{1.29}, \frac{3 - 2.5}{1.29}, \frac{4 - 2.5}{1.29}\right]
$$
  计算后，得到：
$$
  \text{LayerNorm}(x) \approx [-1.16, -0.39, 0.39, 1.16]
$$
  可以看到，层归一化将向量x中的每个元素转换为一个标准化的值，使得向量的均值为0，方差为1。

  残差连接是一种将输入向量和输出向量相加的函数，它的作用是在输出向量中保留输入向量的信息，从而避免信息的丢失。残差连接的公式如下：
$$
  \text{ResidualConnection}(x, y) = x + y
$$
  其中，x表示输入向量，y表示输出向量。残差连接的意义是将输入向量和输出向量看作是两个部分的信息，一个是原始的信息，一个是新的信息，将它们相加，得到一个完整的信息。残差连接的一个特点是，它可以使得神经网络的层数增加，而不会导致梯度消失或爆炸，从而提高模型的深度和复杂度。

  例如，假设我们有一个输入向量x = [1, 2, 3, 4]，和一个输出向量y = [0.5, 0.6, 0.7, 0.8]，我们对它们进行残差连接，得到：
$$
  \text{ResidualConnection}(x, y) = [1, 2, 3, 4] + [0.5, 0.6, 0.7, 0.8]
$$
  计算后，得到：
$$
  \text{ResidualConnection}(x, y) = [1.5, 2.6, 3.7, 4.8]
$$
  可以看到，残差连接将输入向量和输出向量相加，得到一个新的向量，这个向量包含了输入向量和输出向量中的信息，而不会丢失任何信息。

  在多头自注意力机制中，我们对新的输出向量进行层归一化和残差连接，得到最终的输出向量。这样做是为了稳定模型的训练，防止梯度消失或爆炸，同时保留输入向量的信息。这个输出向量的长度与输入向量的长度相同，但是包含了经过多头自注意力机制处理后的信息，从而增强了模型的表达能力。

----



![13](G:\OneDrive - 华南师范大学\文档\平时的截图\13.png)

![12](G:\OneDrive - 华南师范大学\文档\平时的截图\12.png)

![11](G:\OneDrive - 华南师范大学\文档\平时的截图\11.png)

![10](G:\OneDrive - 华南师范大学\文档\平时的截图\10.png)

![9](G:\OneDrive - 华南师范大学\文档\平时的截图\9.png)

![7](G:\OneDrive - 华南师范大学\文档\平时的截图\7.png)

![6](G:\OneDrive - 华南师范大学\文档\平时的截图\6.png)

![5](G:\OneDrive - 华南师范大学\文档\平时的截图\5.png)

![4](G:\OneDrive - 华南师范大学\文档\平时的截图\4.png)

![3](G:\OneDrive - 华南师范大学\文档\平时的截图\3.png)

![2](G:\OneDrive - 华南师范大学\文档\平时的截图\2.png)

![1](G:\OneDrive - 华南师范大学\文档\平时的截图\1.png)

